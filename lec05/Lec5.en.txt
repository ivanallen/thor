Today the TAS are going to be giving a  lecture on concurrency and go basically  this lecture is going to be full of  design patterns and practical tips to  help you with the labs we're going to be  covering briefly the code memory model  the reading which we went over and then  spend most of the lecture talking about  concurrency primitives and go  concurrency patterns and go how you do  things that you will need to do in the  labs and then finally we'll talk through  some debugging tips and techniques and  show you some interesting tools that you  might want to use when debugging the  labs so very briefly on the go memory  model on the reading so why did we  assign this reading well the goal was to  give you some concrete examples of  correct ways to write threaded code and  go so the document like in the second  half of the document has some examples  of correct code and an incorrect code and  how it can go wrong so one thing you  might have noticed in the document is  early on it says if you need to read and  understand this you're being too clever  and we think that that's good advice so  focus on how to write correct code don't  focus way too much on the happens before  relation and being able to reason about  exactly why incorrect code is't correct  like we don't really care we just want  to be able to write correct code and  call it a day  one question that came up in the lecture  questions was like talking about  goroutines in relation to performance  and so we just wanted to say that  goroutines and like in general  concurrency can be used for a couple  different reasons and the reason we use  concurrency in the labs is not  necessarily for performance like we're  not going for parallelism using multiple  cores on a single machine in order to be  able to do more work on the CPU  concurrency gets us something else  besides performance through parallelism  it can get us better expressivity  like we want to write down some ideas  and it happens to be that writing down  code that uses threads is a clean way of  expressing those ideas and so the  takeaway from that is when you use  threads in lab 2 and Beyond don't try to  do fancy things you might do if you're  going for performance especially CPU  performance like we don't care to do  things like using fine-grained locking  or other techniques use basically write  code that's easy to reason about use big  locks to protect large critical sections  and just like don't worry about  performance in the sense of CPU  performance  so with that that's all we're going to  say about the memory model and spend  most of this lecture just talking about  go code and go concurrency patterns and  as we go through these examples feel  free to ask any questions about what's  on the screen or anything else you might  think about so I'm going to start off  talking about concurrency primitives and  go so the first thing is closures this  is something that will almost certainly  be helpful in the labs and this is  related to goroutines so here's this  example program on the screen and what  it does is the main function declares a  bunch of variables and then spawns this  goroutine in here with this go  statement and we noticed that the score  routine is not taking it as an argument  a function call to some function defined  elsewhere but this anonymous function  just defined in line here so this is a  handy pattern this is something called a  closure and one neat thing about this is  that this function that's defined here  can refer to variables from the  enclosing scope so for example this  function can mutate this variable a  that's defined up here or refer to this  wait group that's defined up here so  if we go run this example it does what  you think it does  the wait group dot done here let's the  main thread continue past this point it  prints out this variable which has been  mutated by this concurrently running  thread that finished before this wait  happened so this is a useful pattern to  be able to use one like the reason we're  pointing this out is because you might  have code that looks like this in your  labs very similar to the previous  example except this is code that is  spawning a bunch of threads in a loop  this is useful for example when you want  to send rpcs in parallel right so  like in lab two if you have a candidate  asking for votes you want to ask for  votes from all the followers in parallel  not one after the other because the RPC  is a blocking operation that might take  some time or similarly the leader might  want to send append entries to all the  followers you want to do it in parallel  not in series and so threads are a clean  way to express this idea and so you  might have code that looks kind of like  this at a high level in a for loop you  spawn a bunch of goroutines one thing  to be careful about here this is  something that was talked about in a  previous lecture  is identifier capture and goroutines and  mutation of that identifier in the outer  scope so we see here that we have this  i that's being mutated by this for  loop and then we want to use that value  inside this goroutine and the way  we do that like the correct way of  writing this code is to pass this value i  as an argument to this function and  this function or you can rename it to X  inside here and then use the value  inside and so if we run this program  so here I've kind of stubbed out to send  our RPC thing was actually just prints  out the index this I might be like the  index of the follower trying to send an  RPC to here prints out the numbers 0  through 4 in some order so this is what  we want like send our PCs to all the  followers the reason we're showing you  this code is because there's a variation  of this code which looks really similar  and maybe intuitively you might think it  does the right thing but in fact it  doesn't so in this code the only thing  that's changed is we've gotten rid of  this argument here that we're explicitly  passing and instead we're letting this I  refer to the i from the outer scope so  you might think that when you run this  it does the same thing but in fact in  this particular run it printed 4 5 5 5 5  so this would do the wrong thing and the  reason for this is that this I is being  mutated by this outer scope and by the  time this goroutine ends up actually  executing this line well the for loop  has already changed the value of I so  this doesn't do the right thing so at a  high level if you're spawning goroutines  in a loop just make sure that you use  this pattern here and everything will  work right any questions about that  so it's just like a small gotcha but  we've seen this a whole bunch of times  in office hours so I just wanted to  point this out all right so moving on to  other patterns that you might want to  use in your code oftentimes you want  code that periodically does something a  very simple way to do that is to have a  separate function that in an infinite  loop does something in this case we're  just printing out tick and then use this  time dot sleep to wait for a certain  amount of time so very simple pattern  here  you don't need anything fancier than  this to do something periodically  one modification of this that you might  want is you want to do something  periodically until something happens for  example you might want to start up a  raft here and then periodically send  heartbeats but when we call dot kill on  the raft instance you want to actually  shut down all these goroutines so you  don't have all these random goroutines  still running in the background and so  the pattern for that looks something  like this you have a goroutine that will  run in an infinite loop and do something  and then wait for a little bit and then  you can just have a shared variable  between whatever control thread is going  to decide whether this goroutine should  die or not so in this example we have  this variable done that's a global  variable and what main does is it waits  for while and sets done to true and in  this goroutine that's ticking and doing  work periodically we're just checking  the value of done and if done is set  then we terminate the square-root eeen  and here since done is a shared variable  being mutated and read by multiple  threads we need to make sure that we  guard the use of this with a lock so  that's where this mute outlaw can mute  it unlock comes in for the purpose of  the labs you can actually write  something a little bit simpler than this  so we have this method rf.kill on your  raft instance so you might have code  that looks a little bit more like this  so while you're wrapped instance is not  dead you want to periodically do some  work any questions about that so far  yeah question  does using the locking mechanisms for  channels make it so that any right  stunts any variables and those functions  are to be observed by the fencer would  you need to send done across the channel  okay so let me try to simplify the  question a bit I think the question is  do you need to use locks here can you  use channels instead and are and can you  get away with not using locks and like  what's the difference between nothing  versus channels vs locks is that  basically what you're asking I think the  question is this done does it not need  to be sent across a channel does just  using these locks ensure that this read  here observes the write done by a thread  okay so the answer is yes basically at a  high level if you want to ensure cross  thread communication make sure you use  go synchronization primitives whether  it's channels or locks and condition  variables and so here because of the use  of locks after this thread writes done  and does unlock the next lock that  happens is guaranteed to observe the  writes done before that before this  unlock happened so you have this write  happened and this unlock happened then  one of these locks happens and then the  next done will be guaranteed to observe  that write of true question  that's a good question in this  particular code it doesn't matter but it  would be cleaner to do it so the  question is why don't we do mu dot  unlock here before returning and the  answer is in here there's no more like  the program's done so it doesn't  actually end up mattering but you're  right that like in general we would want  to ensure that we unlock before we  return yeah thanks for pointing that out  so I'm not sure entirely what the  question is but maybe something like can  both of these acquire the lock at the  same time is that the question and we'll  talk a little bit more about locks in  just a moment but at a high level the  semantics of a lock are the lock is  either held by somebody or not held by  somebody and if it's not held by  somebody then if someone calls lock they  have the chance to acquire the lock and  if before they call unlock somebody else  calls lock that other thread is going to  be blocked until the unlock happens then  the lock is free again so at a high  level between the lock and the unlock  for any particular lock like any only a  single thread can be executing what's  called a critical section between the  lock and unlock regions any other  questions  so the question is related to timing  like when you set done equals true and  then you unlock you have no guarantee in  terms of real time like when periodic  will end up being scheduled and observe  this right and actually end up  terminating and so yes if you want to  mean to actually ensure that periodic  has exited for some particular reason  then you could write some code that  communicates back from periodic  acknowledging this but in this  particular case like the only reason we  have the sleep here is just to  demonstrate that the sleep here is just  to demonstrate that tick prints for a  while and then periodic as indeed cancel  it because it stops being printed before  I get my shell prompt back and in  general for a lot of these background  threads like you can just say that you  want to kill them and it doesn't matter  if they're killed within 1 second or  within 2 seconds or one exactly go  schedules it because this thread is  going to just observe this right to done  and then exit do no more works it  doesn't really matter and also another  thing in go is that if you spawn a bunch  of goroutines one of them is the main  goroutine this one here and the way go  works is that if the main goroutine  exits the whole program terminates and  all goroutines are terminated  that's a great question okay so I think  the question is something like why do  you need locks at all like can you just  delete all the locks and then like  looking at this code it looks like okay  main does a right to true at some point  and periodic is repeatedly reading it so  at some point it should observe this  read right well it turns out that like  this is why go has this fancy memory  model and you have this whole thing on  that happens before relation the  compiler is allowed to take this code  and emit a kind of low-level machine  code that does something a little bit  different than what you intuitively  thought would happen here and we can  talk about that in detail offline after  the lecture and office hours but at a  high level I think one rule you can  follow is if you have accesses to shared  variables and you want to be able to  observe them across different threads  you need to be holding a lock before you  read or write those shared variables in  this particular case I think the go  compiler would be allowed to optimize  this to like lift the read of done  outside the four so read this shared  variable once and then if done is false  then set like make the inside be an  infinite loop because like now the way  this thread is written it had uses no  synchronization primitives there's no  mutex lock or unlock no channel sends or  receives and so it's actually not  guaranteed to observe any mutations done  by other concurrently running threads  and if you look on Piazza I've actually  like written a particular go program  that is optimized in the unintuitive way  like it'll produce code that does an  infinite loop even though looking at it  like you might think that oh the obvious  way to compile this code will produce  something that terminates yeah so the  memory model is pretty fancy and it's  really hard to think about why exactly  incorrect programs are incorrect but if  you follow some general rules like whole  blocks before you mutate shared  variables then you can avoid thinking  about some of these nasty issues  any other questions all right so let's  talk a little bit more about mutexes now  so why do you need mutex is at a high  level whenever you have concurrent  access but by different threads to some  shared data you want to ensure that  reads and writes of that data are atomic  so here's one example of program that  declares a counter and then spawns a  goroutine  actually spawns a thousand goroutines  that each update the counter value and  increment it by one and you might think  that looking at this intuitively when I  print out the value of the counter at  the end it should print a thousand but  it turns out that we missed some of the  updates here and in this particular case  it only printed 947 so what's going on  here is that this update here is not  really protected in any way and so these  threads running concurrently can read  the value of counter and update it and  clobber other threads updates of this  value like basically we want to ensure  that this entire section here happens  atomically and so the way you make  blocks of code run atomically are by  using locks and so in this code example  we've fixed this bug we create a lock  and then all these goroutines that  modify this counter value first grab the  lock then update the counter value and  then unlock and we see that we're using  this defer keyword here what this does  is basically the same as putting this  code down here so we grab a lock do some  update then unlock defer is just a nice  way of remembering to do this you might  forget to write the unlock later and so  what defer does is it you can think of  it as like scheduling this to run at the  end of the current function body and so  this is a really common pattern you'll  see for example in your RPC handlers for  the lab so oftentimes RPC handlers will  manipulate either read or write data on  the Raft structure right and those  updates should be synchronized with  other concurrently happening updates and  so oftentimes the pattern for RPC  handles would be like grab the lock  differ unlock and then go do some work  inside so we can see if we run this code  it produces the expected results so it  prints out a thousand and we haven't  lost any of these updates and so what at  a high level what a lock or a mutex can  do is guarantee mutual exclusion for a  region of code which we call a critical  section so in here this is the critical  section and it ensures that none of  these critical sections execute  concurrently with  ones they're all serialized happened one  after another question  yes so this is a good observation this  particular could is actually not  guaranteed to produce a thousand  depending on how thread scheduling end  up ends up happening because all the  main goroutine does is it waits for one  second which is some arbitrary unit of  time and then it prints out the value of  the counter I just want to keep this  example as simple as possible a  different way to write this code that  would be guaranteed to print a thousand  would be to have the main goroutine wait  for all these thousand threads to finish  so you could do this using a wait  group for example but we didn't want to  put two synchronization primitives like  wait groups and mutex is in the same  example so that's why we're at this code  that is like technically incorrect but I  think it still demonstrates the point of  locks any other questions great so at a  very high level you can think of locks  is like you grab the lock you mutate the  shared data and then you unlock so does  this pattern always work well turns out  that that's like a useful starting point  for how to think about locks but it's  not really the complete story so here's  some code this doesn't fit on the screen  but I'll explain it to you we can scroll  through it it basically implements a  bank at a high level so I have Alice and  Bob who both start out with some  balances and then I keep track of what  the total balances like the total amount  of money I store in my bank and then I'm  going to spawn to goroutines that will  transfer money back and forth between  our Alice and Bob so this one  goroutine that a thousand times will  reduce one from Alice and send it to Bob  and concurrently running I have this  other goroutine that in a loop will  reduce one from Bob and send it to Alice  and notice that I have this mutex here  and whenever I manipulate these shared  variables between these two different  threads  I'm always locking the mutex and this  update only happens while this lock is  held right and so is this code correct  or incorrect there actually isn't really  a straightforward answer to that  question it depends on like what are the  semantics of my bank like what behavior  do I expect so I'm going to introduce  another thread here I'll call this one  the audit thread and what this is going  to do is every once in a while I'll  check it check the sum of all the  accounts in my bank and make sure that  the sum is the same as what it started  out as  right click if I only allow transfers  within my bank the total amount should  never change so now given this other  thread so what this does is it grabs the  lock then sums up Alice Plus Bob and  compares it to the total and if it  doesn't match then it says that though  I've observed some violation that my  total is no longer what it should be if  I run this code I actually see that a  whole bunch of times  this concurrently running thread does  indeed observe that Alice Plus Bob is  not equal to the overall sum so what  went wrong here like we're following our  basic rule of whenever we're accessing  data that's shared between threads we  grab a lock it is indeed true that no  updates to these shared variables happen  while the lock is not held exactly so  let me repeat that for everybody to hear  what we intended here was for this  decrement and increment to happen  atomically but instead of what we ended  up writing was code that decrement  atomically and then increments  atomically and so in this particular  code actually like we won't lose money  in the long term like if we let these  threads run and then wait till they  finish and then check the total it will  indeed be what it started out as but  while these are running since this  entire block of code is not atomic we  can temporarily observe these violations  and so at a higher level the way should  think about locking is not just like  locks are to protect access to shared  data but locks are meant to protect  invariants you have some shared data  that multiple people might access and  there's some properties that hold on  that shared data like for example here I  is the programmer decided that I want  this property that alice + Bob should  equal some constant and that should  always be that way I want that property  to hold but then it may be the case that  different threads running concurrently  are making changes to this data and  might temporarily break this invariant  here right like here when I decrement  from Alice temporarily the sum Alice  Plus Bob has changed but then this  thread eventually ends up restoring this  invariant here and so locks are meant to  protect and vary  at a high level you grab a lock then you  do some work that might temporarily  break the invariant but then you restore  the invariant before you release the  lock so nobody can observe these in  progress updates and so the correct way  to write this code is to actually have  less use of lock and unlock  we have lock then we do a bunch of work  and then we unlock and when you run this  code we see no more printouts like this  that we never have this audit thread  observe that the total is not what it  should be all right so that's the right  way to think about locking at kind of a  high level you can think about it as  make sure you grab locks when every  access shared data like that is a rule  but another important rule is locks  protect invariants so grab a lock  manipulate things in a way that might  break the invariants but restore them  afterwards and then release the lock  another way you can think about it is  locks can make regions of code atomic  not just like single statements or  single updates to shared variables any  questions about that great so the next  synchronization primitive we're going to  talk about it something called condition  variables and this is it seems like  there's been a source of confusion from  lab one where we mentioned condition  variables but didn't quite explain them  so we're going to take the time to  explain them to you now and we're going  to do that in the context of an example  that you should all be familiar with  counting votes so remember in lab 2a you  have this pattern where whenever a Raft  peer becomes a candidate it wants to  send out vote requests all of its  followers and eventually the followers  come back to the candidate and say yes  or no like whether or not the candidate  got the vote right and one way we could  write this code is have the candidate in  serial ask peer number one peer number  two peer number three and so on but  that's bad right because we want the  candidate ask all the peers in parallel  so it can quickly win the election when  possible and then there's some other  complexities there like when we ask all  the peers in parallel we don't want to  wait so we get a response from all of  them before making up our mind right  because if a candidate gets a majority  of votes like it doesn't need to wait  till it hears back from everybody else  so this code is kind of complicated in  some ways and so here here's a kind of  stubbed out version of what that vote  counting code might look like  with a little bit of infrastructure to  make it actually run and so here have  this mean goroutine that sets count  which is like the number of yes votes I  got to zero and finish to zero finished  as the number of responses I've gotten  in total and the idea is I want to send  out vote requests in parallel and keep  track of how many yeses I've got and how  many responses I've gotten in general  and then once I know whether I've won  the election or whether I know that I've  lost the election then I can determine  that and move on and like the real raft  code you actually do whatever you need  to do don't step up to a leader or to  step down to a follower after you have  the result from this and so looking at  this code here I'm going to in parallel  spawn say I have ten peers in parallel  spawn ten goroutines  here I pass in this closure here and I'm  gonna do is request a vote and then if I  get the vote I'm going to increment the  count by one and then I'm also going to  increment this finished by one so like  this is a number of yeses this is total  number of responses I've gotten and then  outside here in the main goroutine what  I'm doing is keeping track of this  condition I'm waiting for this condition  to become true that either I have enough  yes votes that I've won the election or  I've heard back from enough peers and I  know that I've lost and so I'm just  going to in a in a loop check to see and  wait until count is greater than or  equal to five or wait until finished is  equal to ten and then after that's the  case I can either determine that I've  lost drive one so does anybody see any  problems with this code given what we  just talked about about mutexes yes  yeah exactly  countin finished aren't protected by  mutexes so one thing we certainly need  to fix here is that whenever we have  shared variables we need to protect  access with new taxes and so that's not  too bad to fix here I declare mutex  that's accessible by everybody and then  in the goroutines I'm launching in  parallel to request votes I'm going to  and this this pattern here is pretty  important I'm going to first request a  vote while I'm not holding the lock and  then after wear that I'm going to grab  the lock and then update these shared  variables and then outside I have the  same patterns as before except I make  sure to lock and unlock between reading  these shared variables so in an infinite  loop I grab the lock and check to see if  the results of the election have been  determined by this point and if not I'm  going to keep running in this infinite  loop otherwise I'll unlock and then do  what I need to do outside of here and so  if I run this example whoops it seems to  work and this is actually like a correct  implementation it does the right thing  but there's some problems with it so can  anybody recognize any problems with this  implementation I'll give you a hint this  code is not as nice as it could be  so not quite it's going to wait for  exactly the right amount of time the  issue here is that it's busy waiting  what it's doing is in a very tight loop  it's grabbing the lock checking this  condition unlocking grabbing this lock  checking this condition unlocking and  it's going to burn up 100% CPU on one  core while it's doing this so this code  is correct but it's like at a high level  we don't care about efficiency like CPU  efficiency for the purpose of the labs  but if you're using a hundred percent of  one core you might actually slow down  the rest of your program enough that it  won't make progress and so that's why  this pattern is bad that we're burning  up a hundred percent CPU waiting for  some condition to become true right so  does anybody have any ideas for how we  could fix this so here's one simple  solution  I will change a single line of code all  I've added here is wait for 50  milliseconds and so this is a correct  transformation of that program and it  kind of seems to solve the problem right  like before I was burning up a hundred  percent CPU now only once every 50  milliseconds I'm going to briefly wake  up check this condition and go back to  sleep  if it doesn't hold and so this is like  basically a working solution any  questions so this kind of sort of works  but one thing you should always be aware  of whenever you write code is magic  constants why is this 50 milliseconds  why not a different number like whenever  you have an arbitrary number in your  code it's a sign that you're doing  something that's not quite right or not  quite as clean as it could be and so it  turns out that there's a concurrency  primitive designed to solve exactly this  problem of I have some threads running  concurrently that are making updates to  some shared data and then I have another  thread that's waiting for some property  some condition on that shared data to  become true and until that condition  becomes true the thread is just going to  wait there's a tool designed exactly to  solve this problem and that's a tool  called a condition variable and the way  you use a condition variable is the  pattern basically looks like this so we  have our lock from earlier condition  variables are associated with locks so  we have some shared data some a lock  that protects that shared data and then  we have this condition variable that is  given a pointer to the lock when it's  initialized and we're going to use this  condition variable for kind of  coordinating when a certain condition  some property on that shared data when  that becomes true and the way we modify  our code is like we have two places one  we're making changes to that data which  might make the condition become true and  then we have another place where we're  waiting for that condition to become  true and the general pattern is whenever  we do something that changes the data we  call a conduct broadcast and we do this  while holding the lock and then on the  other side where we're waiting for some  condition on that share data to become  true we call cond dot wait and so what  this does is like let's think about what  happens in the mean thread for a moment  the main thread grabs the lock it checks  this condition suppose it's false it  calls cond dow wait what this will do is  it will atomically you can think of it  as it'll release the lock in order to  let other people make progress and it'll  add its thread like it'll add itself to  a like list of people who are waiting on  this condition variable then  concurrently one of these threads might  be able to acquire the lock after it's  gotten a vote and then it manipulates  these variables and then it calls  cond dot broadcast what that does is it  wakes up whoever's waiting on the  condition variable and so once this  thread unlocks the mutex this one what  do we want as it's returning from wait  we'll reacquire the mutex and then  return to the top of this for loop which  is checking this condition so this  broadcast wakes up whoever's waiting at  this wait and so this avoids having to  have that time dot sleep for some  arbitrary amount of time like this  thread that's waiting for some condition  to become true only gets woken up when  something changes that might make that  condition become true right like if you  think about these threads if they're  very slow and they don't call cond dot  broadcast for a long time this one will  just be waiting it won't be like  periodically waking up and checking some  condition that can't have changed  because nobody else manipulated their  shared data so any questions about this  pattern yeah  so that's a great question I think  you're referring to something called the  lost wake up problem and this is a topic  in operating systems and we won't talk  about it in detail now there feel free  to ask me after lecture but at a high  level you can avoid funny race  conditions that might happen between  wait and broadcast by following the  particular pattern I'm showing here and  I'll show you an abstracted version of  this pattern in a moment basically the  pattern is for the side that might make  changes that will change the outcome of  the condition test you always lock then  manipulate the data then call broadcast  and call unlock afterwards so the  broadcast must be called while holding  the lock similarly when you're checking  the condition you grab the lock then  you're always checking the condition in  a loop and then inside so when that  condition is false you call Condit wait  this is only called while you're holding  the lock and it atomically releases the  lock and kind of schedule like puts  itself in a list of waiting threads and  then as waits returning so as we like  return from this wait call and then go  back to the top of this for loop it will  reacquire the lock so this check will  only happen while holding the lock and  then so outside of this we still have  the lock here and we unlock after we're  done doing whatever we need to do here  at a high level this pattern looks like  this so we have one thread or some  number of threads doing something that  might affect the condition so they're  going to grab a lock do the thing call  broadcast then call unlock and on the  other side we have some thread that's  waiting for some condition to become  true the pattern there it looks like we  grab the lock then in a while loop while  the condition is false we wait and so  then we know that when we get past this  while loop now the condition is true and  we're holding the lock and we can do  whatever we need to do here and then  finally we call unlock so we can talk  about all the things that might go wrong  if you violate one of these rules like  after lecture if you're interested but  at a high level if you follow this  pattern then you won't need to deal with  those issues so any questions about that  yeah  so that's a great question  when do you use broadcast versus when do  use signals so converse have three  methods on them one is wait for the  waiting side and then on the other side  you can use signal or broadcast and the  semantics of those are signal wait wakes  up exactly one waiter like one thread  that may be waiting  whereas broadcast wakes up everybody  who's waiting and they'll all reach out  like they'll all try to grab the law can  recheck the condition and only one of  them will proceed because only one of  them will hold lock until it gets  past this point I think for the purpose  of this class always use broadcast never  use signal if you follow this pattern  and just like don't use signal and  always use broadcast your code will work  I think you can stick think of signal as  something used for efficiency and we  don't really care about that level of  CPU efficiency in the labs for this  class  any more questions ok so the final topic  we're going to cover in terms of go  concurrency primitives is channels so  two high level channels are like a queue  like synchronization primitive but they  don't behave quite like cues in the  intuitive sense like I think some people  think of channels is like there's this  data structure we can sticks that stick  things in and eventually someone will  pull those things out but in fact  channels have no queuing capacity they  have no internal storage basically  channels are synchronous if you have to  goroutines that are going to send and  receive on a channel if someone tries to  send on the channel while nobody's  receiving that thread will block until  somebody's ready to receive and at that  point synchronously it will exchange  that data over to the receiver and the  same is true the other direction if  someone tries to receive from a channel  while nobody's sending that receive will  block until there's another goroutine  that's about to send on the channel and  that send will happen synchronously so  here's a little demo program that  demonstrates this here I have a I  declare channel and then I spawn a go  routine that waits for a second and then  sent and then receives from a channel  and then in my main goroutine I keep  track of the time then I send on the  channel so I just put some dummy data  into the channel and then I'm going to  print out how long the send took  and if you think of channels as cues  with internal storage capacity you might  think of this thing as completing very  fast but that's not how channels work  this send is going to block until this  receive happens and this one happened  till this one second is the elapsed and  so from here to here  we're actually blocked in the main  goroutine for one whole second alright so  don't think of channels as queues think  of them as this synchronous like the  synchronous communication mechanism  another example that'll make this really  obvious is here we have a goroutine that  creates a channel then sends on the  channel and tries receiving from it  doesn't anybody know what'll happen when  I try running this  I think the file name might give it away  yeah exactly the send is going to block  till somebody's ready to receive but  there is no receiver and go actually  detects this condition if all your  threads are sleeping it to text this is  a deadlock condition and it'll actually  crash but you can have more subtle bugs  where if you have some other thread like  off doing something if I spawn this go  routine that you know for loop does  nothing and I try running this program  again now it goes deadlock detector  won't notice that all threads are not  doing any use will work like there's one  thread running it's just this is never  receiving and we can tell by looking at  this program that it'll never terminate  but here it just looks like it hangs so  if you're not careful with channels you  can get these subtle bugs where you have  double X as a result yeah yeah exactly  there's no data nobody's sending on this  channel so this is gonna block here it's  never gonna get to this line  yeah so channels as you pointed out  can't really be used just within a  single goroutine it doesn't really make  sense because in order to send or in  order to receive there has to be another  goroutine doing the opposite action at  the same time so if there isn't you're  just gonna block forever and then that  chant but thread will no longer do any  useful work yeah sends wait for receives  receives wait for signs and it happens  synchronously once there's both the  sender and receiver present what I  talked about so far is unbuffered  channels I was going to avoid talking  about buffered channels because there  are very few problems that they're  actually useful for solving so buffered  channels can take in a capacity and then  you can think of it as it's just switch  this to so here's a buffered channel  with a capacity of one this program does  terminate because buffered channels are  like they have some internal storage  space and until that space fills up  sends are non blocking because they can  just put that data in the internal  storage space but once the channel does  fill up then it does behave like a  non-buffer channel in the sense that further  sends will block until there's a receive  to make space in the channel but I think  at a high level we should avoid buffered  channels because they basically don't  solve any problems and another path and  other things should be thinking about is  whenever you to make up arbitrary  numbers like this one here to make your  code work you're probably doing  something wrong yeah  so I think this is a question about  terminology like what exactly does  deadlock mean into this count as a  deadlock like yes this counts as a  deadlock like no useful progress will be  made here like this these threads are  just stuck forever  any other questions so what our channel  is useful for I think channels are  useful for a small set of things like  for example I think for producer  consumer queues sort of situations like  here I have a program that makes a  channel and this spawns a bunch of  goroutines that are going to be doing  some work like say they're competing  some result in producing some data and I  have a bunch of these goroutines  running in parallel and I want to  collect all that data as it comes in and  do something with it  so this do work thing just like waits  for a bit and produces a random number  and in the main goroutine I'm going to  continuously receive on this channel and  print it out like this is a great use of  channels another good use of channels is  to achieve something similar to what  wait groups do so rather than use a wait  group suppose I want to spawn a bunch of  threads and wait till they're all done  doing something one way to do that is to  create a channel and then I spawn a  bunch of threads and know how many  threads I've spawned so five goroutines  created here they're going to do  something and then send on this channel  when they're done and then in the main  goroutine I can just receive from that  channel the same number of times and  this has the same effect as a wait group  so question so what exactly is the  question  [Music]  so the question is here could you use a  buffered channel with a capacity of five  because you're waiting for five receives  I think in this particular case yes that  would have the equivalent effect but I  think there's not really a reason to do  that  and I think at a high level in your code  you should avoid buffer channels and  also maybe even channels unless you  think very hard about what you're doing  yeah so what is a wait group I think  we covered this in a previous lecture  and I talked about it very briefly today  but I do have an example of wait  groups so a wait group is a yet  another synchronization primitive  provided by go in the sync package and  it kind of does what his name advertises  like it lets you wait for a certain  number of threads to be done the way it  works is you call wait group dot add  and that basically increments some  internal counter and then when you call  wait group dot wait it waits till  done has been called as many times as add  was called so this code is basically the  same as the code I just showed you that  was using a channel except this is using  wait group they have the exact same  effect you can use either one yeah  so the question here is about race  conditions I think like what happens if  this add doesn't happen fast enough  before this wait happens or something  like that well so here notice that the  pattern here is we call wait group  data outside of this goroutine and it's  called before spawning this goroutine  so this happens first this happens next  and so we'll never have the situation  we're done happens after this add happens  for this particular routine how's this  implemented by the compiler and I will  not talk about that now but talk to me  after class or in office hours but I  think for the purposes class like you  need to know the API for these things  not the implementation all right and so  I think that's basically all I have on  go concurrency primitives so one final  thought is on channels like channels are  good for a specific set of things like I  just showed you the producer consumer  queue or like implementing something  like wait groups but I think when you  try to do fancier things with them like  if you want to say like kick another go  routine that may or may not be waiting  for you to be like woken up that's a  kind of tricky thing to do with channels  there's also a bunch of other ways to  shoot yourself in the foot with them I'm  going to avoid showing you examples of  bad code with channels just because it's  not useful to see but I personally avoid  using channels for the most part and  just use shared memory and mutexes and  condition variables and set and I  personally find those much easier to  reason about so feel free to use  channels for when they make sense but if  anything looks especially awkward to do  with channels like just use mutexes and  condition variables and they're probably  a better tool yeah  so the question is with the difference  between this producer-consumer pattern  here in a thread-safe FIFO I think  they're kind of equivalent like you  could do this with the thread-safe FIFO  and it like that is basically what a  like buffered channel is roughly if  you're in queueing things in  dequeueing things like if you want this  line to finish and have this thread go  do something else while that data sits  there in a queue rather than this  goroutine waiting to send it then a  buffered channel might make sense but I  think at least in the lab you will not  have a pattern like that all right so  next Fabian's going to talk about more  rapidly related stuff do you need this  all right can you all hear me is this  working yeah all right so yeah basically  I'm going to show you two bugs that we  commonly see in people's raft  implementations there's a lot of bugs  that are pretty common but I'm just  going to focus on two of them so in this  first example we sort of have a start of  a raft implementation for that's sort of  like what you might see for to a just  the beginnings of one  so in our raft state we have primarily  the current status of the raft pier  either follower candidate or leader and  we have these two state variables that  were keeping track of the current term  and who we voted for in the current term  so I'm I want us to focus though on  these two functions AttemptElection and  CallRequestVote so in AttemptElection we're  just going to set our state to candidate  increment our current term vote for  ourselves and then start sending out  request votes to all of our raft peers  and so this is similar to some of the  patterns that Anish showed where we're  going to loop through our peers and then  for each one in a goroutines separately  call this CallRequestVote function in  order to actually send an RPC to that  peer  alright so in CallRequestVote we're  going to acquire the lock prepare  arguments for our request vote RPC call  based on by setting it to the current  term and then actually perform the RPC  call over here and finally based on the  response we will reply back to this this  AttemptElection function and the  AttemptElection function eventually  should tally up the votes to see if it  got a majority of the votes and can  become leader so what happens when we  run this code so in theory what we might  expect to happen is for so there's  going to be some code that's going to  spawn a few graph spears and actually  try to attempt elections on them and  what should happen are we just start  collecting votes from other peers and  then we're not actually going to tally  them up  but hopefully nothing weird goes wrong  but actually something is going to go  wrong here and we actually activated  goes deadlock detector and somehow we  ran into a deadlock so let's see what  happened for now let's focus on what's  going on with the server zero so server  zero it says it starts attempting an  election at term one that's just  starting the AttemptElection function  it will acquire the lock set some of the  set some stuff up for performing the  election and then unlock then it's going  to send out a request vote RPC to server  two it finishes processing that request  vote RPC over here so we're just  printing right before and after we  actually send out the RPC and then it  sends out a request vote RPC to server  one but after that it never we never  actually see it finish sending the  request vote RPC so it's actually stuck  in this function call waiting for the  RPC response from server 1 all right now  let's look at what's everyone's doing so  it's it's pretty much the same thing it  sends a request vote I received a server  two that that succeeds it finishes  processing that request vote the  response from server 2 then it sends  this RPC to zero and now what's actually  happening is 0 & 1 are sort of waiting  for the RPC responses from each other  they both sent out an RPC call but not  yet got the response yet and that's  actually sort of the cause of our  deadlock so really what's the reason  that we're dead locking is because we're  holding this lock through our RPC calls  over here in the core requests vote  function we acquire our mutex associated  with our raft peer and we only unlock at  the end of this function so throughout  this entire function we're holding the  lock including when we try to contact  our peer to get the vote and later when  we handle this request vote RPC we  actually only see it at the beginning of  this function in the handler we're also  trying to acquire the lock but we never  actually succeed in acquiring the lock  so just to make this a little bit more  clear the the sort of order of  operations  is happening is in CallRequestVote  server zero is first going to acquire  the lock and send an RPC call to server  one and then simultaneously and  separately server one is going to do the  same thing it's going to enter its call  request vote function acquire the lock  and send this RPC call to server zero  now in server zeros handler and server  ones handler they're trying to acquire  the lock but they can't because they  already are acquiring the lock and  trying to send the RPC call to each  other and that that's actually what's  leading to the deadlock situation so to  solve this basically we want you to not  hold locks through RPC calls and that's  the solution to this problem in fact we  don't need the lock here at all instead  of trying to read the current term when  we enter this CallRequestVote function  we can pass this as an argument here  save the term when we had acquired the  lock earlier in this AttemptElection  and just passed this as a as a variable  to CallRequestVote so that actually  removes the need to acquire the lock at  all in CallRequestVote alternatively  we could lock while we're preparing the  arguments and then unlock before  actually performing the call and then if  we need to to process the reply we could  lock again afterwards so it's just make  sure to unlock before making it  obviously call and then if you need to  you can acquire the lock again so now if  I save this then so it's still  activating the deadlock detector but  that's actually just because we're not  doing anything at the end but now it's  actually working  we finished sending the request votes on  both sides and all the operations that  we wanted to complete are complete all  right any questions about this example  yeah so not it's sort of so you might  need to use locks when you are preparing  the arguments or processing the response  but yeah you shouldn't hold a lock  through the RPC call while you're  waiting for the other peer to respond  and there's actually another reason to  that in addition to deadlock the other  problem is that in some tests we're  going to sort of have this unreliable  network that could delay some of your  RPC messages potentially by like 50  milliseconds and in that case if you  hold the lock through an RPC call then  any other operation that you try to do  during that 50 milliseconds won't be  able to complete until that RPC response  is received so that that's another issue  that you might run into if you hold the  lock so it's both to make things more  efficient and to avoid these potential  deadlock situations  all right so just one more example this  is again using a similar draft  implementation so again in our raft  state we're going to be keeping track of  whether a fuller candidate leader and  then also these two state variables in  this example I want you to focus on this  AttemptElection function so now we've  first implemented the change that I just  showed you to store the term here and  pass it as a variable to our function  that collects the request votes but  additionally we've implemented some  functionality to add up the votes so  what we'll do is we'll create a local  variable to count the votes and whenever  we get a vote if the vote was not  granted  we'll return immediately from this go  routine where we're processing the boat  otherwise we'll acquire the lock before  editing this shared local variable to  count up the votes and then if we did  not get a majority of the votes will  return immediately otherwise we'll make  ourselves the leader so as with the  other example I mean initially if you  look at this if I look at this like it  seems reasonable but let's see if  anything can go wrong all right so this  is the log output from one run and one  thing you might notice is that we've  actually elected two leaders on the same  term so server zero  it was elected made itself a leader on  term two and server one did as well it's  okay to have a leader elected on  different terms but here where we have  one on the same term that that should  never happen alright so how did this  actually come up so let's start from the  top so at the beginning server zero  actually attempted an election at term  one not turn two and it got its votes  from both of the other peers but for  whatever reason perhaps because those  reply messages from those peers were  delayed it didn't actually process its  process those votes until later and in  between receiving it like in between  attempting the election and finishing  the election server one also decided to  attempt an election perhaps because  because of server zero was delayed so  much server one might  actually ran into the election timeout  and then started its own election and it  started it on term 2 because it couldn't  have been termed 1 because it already  voted for server 0 on on term 1 over  here  okay so then server 1 sends out its own  request votes 2 servers 2 and 0 at term   for server 1 that's fine but server 0  also votes for server 1 this is actually  also fine because server one is asking  server 0 for a vote on a higher term and  so what server 0 should do is if you  remember from the spec it should set its  current term to that term in the request  for RPC message to term 2 and also  revert itself to a follower instead of a  candidate alright finally so the real  problem is that on this line where  server 0 although it really got enough  votes on term 1 it made itself a leader  on term - so the reason so one  explanation for why this is happening is  because in between where we set up the  election our attempt for the election  and where we actually process the votes  some other things are happening input in  this case we're actually voting for  someone else in between and so we're no  longer on term 1 where we thought we  started the election we're now on term 2  and so we just need a double check that  because we don't have the lock while  we're performing the RPC calls which is  important for its own reasons now some  things might have changed and we need to  double check that what we assume is true  when we're setting ourselves to the  leader is still true so one way to solve  this that there's a few different ways  like to solve this like you could  imagine not voting for others while  we're in the middle of attempting an  election but in this case the simplest  way to solve this at least in this  implementation is to just double check  that we're still on the same term and  we're still a candidate we haven't  reverted to a follower so actually one  thing I want to show you is if we do  print out our state over here then we do  see that server 0 became a follower but  it's still setting itself to a leader on  this line  so yeah we can just check for that if  we're not a candidate or the current  term doesn't match the term which we  started the election then let's just  quit and if we do that then  so everyone becomes a leader and we  never cease over zero become leader so  the problem solved any question yeah  yeah I think I think that would I  because we would not if the term is  higher now than actually no it would it  might not be sufficient because we might  have attempted another election it  depends on your implementation but it's  possible that you could have attempted  another election on a higher term  afterwards all we know that's the same  thing right yeah it would not be  sufficient to only check the state but I  think you're right if you only check the  term then it is sufficient all right any  other questions all right so yeah that's  it for this part she's going to show you  some more examples of actually debugging  some of these draft implementations  hi can you all hear me yeah  is it not  okay so in my section I'm gonna walk you  through how I would be but if you have  like a bug in your raft implementation  so I prepare a couple of buggy raft code  and I just try to walk you through it so  first I'm gonna go into my first  buggy implementation and if I run the  test here so for this one it doesn't  print anything it just gets started and  it's gonna be here forever and let's  assume that I have no idea why there's  happening  the first thing that I want to find out  is where it gets started and we we do  have a good tool for that which printf  but in the stop code if you go to  youtube go we have a function called the  printf this is just a nice wrapper  around the block printf with the  debugger able to enable or disable the  locking messages so I'm gonna enable  that and go back to my raft code so  first of all when i when when there  there's something that's bug happening I  always go check if the code actually  actually initialize raft server so here  I'll just clean  okay so here if I run the test again  then now I know that there are three  servers that get initialized so this  files is okay but like there's nowhere  where the bug is happening so I'll just  go deeper into the hood just to find  where it gets stuck so now if you see  the code we are calling the leader a  election so I'm gonna go to that  function and just to make faster I'll  try to check if it kicks off some  election  that part still fine so we we try to go  for now here we are in the election I'll  see if there's so we actually send the  request vote to some other servers  now we kind of have like more idea of  where guests are because it's not  printing that some sorry that kicks off  the election are not sending the request  words so I would go back for her just to  see where customers like I always tried  here prin if if we call some function  I I  I was always double shake if it actually  go into the function so now I'm going to  say that this service is at the start of  the election  and that works so now we have an idea of  like the bug should be between here and  here so we are trying to minimize the  scope of the code that's causing the bug  let's say if I print something here  and it does it doesn't get there so I  move it up let's say here still not  there  now it's there so the bug is probably in  this function and I just go check so  here the problem is that I'm trying to  acquire a lock where I actually do have  the lock so it's gonna be a deadlock so  that's how I will find their first bug  using the DPrintf and it's it's nice  to use the printf because you can like  just turn off the debugging print and  have a nice test output with our audit  debugging if you want it so that's how I  would use it DPrintf to try to like  handle a bug in your code and for this  example there's actually another trick  to help you find this kind of deadlock  so if you press ctrl + backslash you can  see in the bottle but bottom left that I  press like control and backslash this  this command will send a signal quit  today  go program and by default it will  handles the the quiz signal and quit all  the goroutines and print audio strike  the stack rates so now this like Chico  up here like this way it gets touched  and then there are gonna be a couple  functions printing here  just trying to go through all the traces  yes so it's actually showing that the  function that's causing the problem is  the cover to candidate so that's another  wait you've to find out where the day  locks are I can remove all this  and now it works so that's the first  example that I want to go through second  thing that you want it you want to do  before you submit your labs is to turn  the race  flag on when you do the test the way to  do that is just to add -race before  -run and here because my implement  implementation doesn't have any races  so it's not going to tell you anything  but this just be careful about this  because it's not a proof that you don't  have any really it's just that it cannot  detect races for you I'm going to run  the same command again with the red flag  but now this time that's actually race  going on in my implementation so it's  gonna yell at you that there's some  deliveries going on in your code  I'm quitting that and let's see like how  useful is the warning are so I'm gonna  go to my second implementation with  Raft code and here  let's look at this race so it's telling  us that there's a wait going on at the  line   wait on probably Thursday here and  there's also a right line 412 which  is Thursday so  I'm going to this line again  and now we kind of know that this this  radiation is protected by a lock so the  risk flies actually wanting us and  helping us to find out bug on on this  database that we have so the fake it's  gonna be just you lock this and unlock  it and that should solve the problem  so at this place we kind of know how to  basic like do some basic debugging does  anyone have any question no okay yeah so  I'm going to go to the third one which  is going to be more difficult to find  a bug I'm going to test the run the  centers and now I am I actually have  some debugging messages in there already  and just see that I also have a  debugging message with the test action  there's something you might want to  consider doing if you go into the test  clip here  you can just see how the test would run  and then there are some actions that the  test clip is gonna do to make your code  fail and it's usually a good idea to  print out where that action is happening  in your actual debugging message so you  can guess what is happening like where  the bug is happening in which phase of  the test if that make sense so now it's  like I was doing fine in the first case  I passed I passed the fail but I'm  failing their second test and here the  Test section is to found one as a little  one so I'm passing this the test until  this and if you go to I'm actually  passing until the leader two rejoins so  this can give you a nice idea of how the  test is working and just to help you  have a better case as where the bondage  is in your code so now let's look at the  debugging messages  so it's least it seems like when leader 2  rejoined it becomes a follower and we  have a new leader  so that looks fine to me and we probably  need more debugging messages instead of  just their state changes so I am going  to add some more my first case that when  one becomes a leader it might not be  doing what a leader should you correctly  so we got stuck  so you might could after we cover it as  eventually there I have a goroutine  call operate leader  there's just sending heartbeat to the all set  to the all servers so I'm gonna print  some stuff here saying heartbeat  cheers  away  so to become a leader it sends the the  first heartbeat to each server and one still  tries to send heartbeat to the new leader  and then one becomes a follower so this  doesn't look like to be a problem now  I'm gonna check if the other service  receive heartbeat correctly  it's taking away with I'm trying to  finish this yeah so to becomes a leader  to sends heartbeat but no one receive a  heartbeat form - so if I go to the same  opinion tree I actually hold the law to  the RPC Hall which is the problem that  Fabian went to in the last section so  that's that's the problem that I need to  fix so what I should do is to a log here  and then  lock again here and that should work  we pass and then there are couple things  that you might want to do when you test  your rough implementation so that's  actually script to run the test in  imperial and I can show you how I how we  can use how we can use it this creep is  in the inner peer support some someone  make a point about it and here's how we  can use the script so you run the script  specify the number of the test  personally I do like a 1000 but that  depends on your preference this is the  number of course that you wanna run the  test at the same time and then here's  the test and if you run the script then  if you show you that's like we have run  four tests so far all are working fine  and it's gonna keep going like that so  that's how I would go about debugging  rough implementation and you are all  welcome to come to office hours when you  need help   